{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Julia MNIST\n",
    "\n",
    "In here I share how to run MNIST using Julia & flux."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Pkg;Pkg.add(\"MLDatasets\");Pkg.add(\"Flux\");Pkg.add(\"Statistics\");Pkg.add(\"ProgressMeter\");Pkg.add(\"Plots\");Pkg.add(\"Metal\");Pkg.add(\"GZip\");Pkg.add(\"MLUtils\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "using MLDatasets, Flux, Statistics, ProgressMeter, Plots, Metal, MLUtils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(features = Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0;;; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0;;; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0;;; … ;;; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0;;; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0;;; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], targets = [7, 2, 1, 0, 4, 1, 4, 9, 5, 9  …  7, 8, 9, 0, 1, 2, 3, 4, 5, 6])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "using MLDatasets: MNIST\n",
    "# load full training set\n",
    "train_x, train_y = MNIST(split=:train, dir=\"mnist\")[:]\n",
    "\n",
    "# load full test set\n",
    "test_x,  test_y  = MNIST(split=:test, dir=\"mnist\")[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "313-element DataLoader(::Tuple{Array{Float32, 3}, OneHotArrays.OneHotMatrix{UInt32, Vector{UInt32}}}, batchsize=32)\n",
       "  with first element:\n",
       "  (28×28×32 Array{Float32, 3}, 10×32 OneHotMatrix(::Vector{UInt32}) with eltype Bool,)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_y = Flux.onehotbatch(train_y, 0:9)\n",
    "train_x = Float32.(train_x)\n",
    "test_y = Flux.onehotbatch(test_y, 0:9)\n",
    "test_x = Float32.(test_x)\n",
    "\n",
    "train_loader = DataLoader((train_x, train_y), batchsize=32)\n",
    "test_loader = DataLoader((test_x, test_y), batchsize=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flux.gpu_backend!(\"Metal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Chain(\n",
       "  Flux.flatten,\n",
       "  Dense(784 => 128, relu),              \u001b[90m# 100_480 parameters\u001b[39m\n",
       "  Dense(128 => 10),                     \u001b[90m# 1_290 parameters\u001b[39m\n",
       "  NNlib.softmax,\n",
       ") \u001b[90m                  # Total: 4 arrays, \u001b[39m101_770 parameters, 397.789 KiB."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = Chain(\n",
    "    Flux.flatten,\n",
    "    Dense(784 => 128, relu),\n",
    "    Dense(128 => 10),\n",
    "    Flux.softmax\n",
    ")# |> gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(layers = ((), (weight = \u001b[32mLeaf(Adam(0.01, (0.9, 0.999), 1.0e-8), \u001b[39m(Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, bias = \u001b[32mLeaf(Adam(0.01, (0.9, 0.999), 1.0e-8), \u001b[39m(Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, σ = ()), (weight = \u001b[32mLeaf(Adam(0.01, (0.9, 0.999), 1.0e-8), \u001b[39m(Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, bias = \u001b[32mLeaf(Adam(0.01, (0.9, 0.999), 1.0e-8), \u001b[39m(Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, σ = ()), ()),)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "optim = Flux.setup(Flux.Adam(0.01), model)  # will store optimiser momentum, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in 1:10\n",
    "    Flux.train!(model, train_loader, optim) do m, x, y\n",
    "        y_hat = m(x)\n",
    "        Flux.logitcrossentropy(y_hat, y)\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9384"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "correct = 0\n",
    "for (x, y) in test_loader\n",
    "    y_hat = model(x)\n",
    "    correct += sum(findmax(y_hat, dims=1)[2] .== findmax(y, dims=1)[2])\n",
    "end\n",
    "\n",
    "correct / size(test_y)[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.10.4",
   "language": "julia",
   "name": "julia-1.10"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
